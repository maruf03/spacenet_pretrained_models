{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tiWnl9UrwJf","executionInfo":{"status":"ok","timestamp":1650721986163,"user_tz":-360,"elapsed":21760,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"479d3828-d14e-4dc0-a20d-166a7a63ddf9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!curl https://colab.chainer.org/install | sh -"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlamrm6Zq4da","executionInfo":{"status":"ok","timestamp":1650722023923,"user_tz":-360,"elapsed":35690,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"9bfb3664-ba3e-416b-f5e1-5700c28b8928"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  1580  100  1580    0     0     89      0  0:00:17  0:00:17 --:--:--   355\n","+ apt -y -q install cuda-libraries-dev-10-0\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n","0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n","+ pip install -q cupy-cuda100  chainer \n","\u001b[K     |████████████████████████████████| 58.9 MB 1.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.0 MB 36.7 MB/s \n","\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","+ set +ex\n","Installation succeeded!\n"]}]},{"cell_type":"code","source":["from __future__ import division\n","\n","import numpy as np\n","import six\n","\n","\n","def calc_semantic_segmentation_confusion(pred_labels, gt_labels):\n","    pred_labels = iter(pred_labels)\n","    gt_labels = iter(gt_labels)\n","\n","    n_class = 0\n","    confusion = np.zeros((n_class, n_class), dtype=np.int64)\n","    for pred_label, gt_label in six.moves.zip(pred_labels, gt_labels):\n","        if pred_label.ndim != 2 or gt_label.ndim != 2:\n","            raise ValueError('ndim of labels should be two.')\n","        if pred_label.shape != gt_label.shape:\n","            raise ValueError('Shape of ground truth and prediction should'\n","                             ' be same.')\n","        pred_label = pred_label.flatten()\n","        gt_label = gt_label.flatten()\n","\n","        # Dynamically expand the confusion matrix if necessary.\n","        lb_max = np.max((pred_label, gt_label))\n","        if lb_max >= n_class:\n","            expanded_confusion = np.zeros(\n","                (lb_max + 1, lb_max + 1), dtype=np.int64)\n","            expanded_confusion[0:n_class, 0:n_class] = confusion\n","\n","            n_class = lb_max + 1\n","            confusion = expanded_confusion\n","\n","        # Count statistics from valid pixels.\n","        mask = gt_label >= 0\n","        confusion += np.bincount(\n","            n_class * gt_label[mask].astype(int) +\n","            pred_label[mask], minlength=n_class**2).reshape((n_class, n_class))\n","\n","    for iter_ in (pred_labels, gt_labels):\n","        # This code assumes any iterator does not contain None as its items.\n","        if next(iter_, None) is not None:\n","            raise ValueError('Length of input iterables need to be same')\n","    return confusion\n","\n","\n","def calc_semantic_segmentation_iou(confusion):\n","    iou_denominator = (confusion.sum(axis=1) + confusion.sum(axis=0) -\n","                       np.diag(confusion))\n","    iou = np.diag(confusion) / iou_denominator\n","    return iou\n","\n","\n","def eval_semantic_segmentation(pred_labels, gt_labels):\n","    confusion = calc_semantic_segmentation_confusion(\n","        pred_labels, gt_labels)\n","    iou = calc_semantic_segmentation_iou(confusion)\n","    pixel_accuracy = np.diag(confusion).sum() / confusion.sum()\n","    class_accuracy = np.diag(confusion) / np.sum(confusion, axis=1)\n","\n","    return {'iou': iou, 'miou': np.nanmean(iou),\n","            'pixel_accuracy': pixel_accuracy,\n","            'class_accuracy': class_accuracy,\n","            'mean_class_accuracy': np.nanmean(class_accuracy)}"],"metadata":{"id":"EZhwEsL4qFrM","executionInfo":{"status":"ok","timestamp":1650722029356,"user_tz":-360,"elapsed":8,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import chainer\n","import chainer.functions as F\n","import chainer.links as L\n","\n","\n","class UNet(chainer.Chain):\n","\n","    def __init__(self, class_num=2, ignore_label=255):\n","\n","        self.__class_num = class_num\n","        self.__ignore_label = ignore_label\n","\n","        super(UNet, self).__init__(\n","            c0=L.Convolution2D(3, 32, 3, 1, 1),\n","            c1=L.Convolution2D(32, 64, 4, 2, 1),\n","            c2=L.Convolution2D(64, 64, 3, 1, 1),\n","            c3=L.Convolution2D(64, 128, 4, 2, 1),\n","            c4=L.Convolution2D(128, 128, 3, 1, 1),\n","            c5=L.Convolution2D(128, 256, 4, 2, 1),\n","            c6=L.Convolution2D(256, 256, 3, 1, 1),\n","            c7=L.Convolution2D(256, 512, 4, 2, 1),\n","            c8=L.Convolution2D(512, 512, 3, 1, 1),\n","\n","            dc8=L.Deconvolution2D(1024, 512, 4, 2, 1),\n","            dc7=L.Convolution2D(512, 256, 3, 1, 1),\n","            dc6=L.Deconvolution2D(512, 256, 4, 2, 1),\n","            dc5=L.Convolution2D(256, 128, 3, 1, 1),\n","            dc4=L.Deconvolution2D(256, 128, 4, 2, 1),\n","            dc3=L.Convolution2D(128, 64, 3, 1, 1),\n","            dc2=L.Deconvolution2D(128, 64, 4, 2, 1),\n","            dc1=L.Convolution2D(64, 32, 3, 1, 1),\n","            dc0=L.Convolution2D(64, class_num, 3, 1, 1),\n","\n","            bnc0=L.BatchNormalization(32),\n","            bnc1=L.BatchNormalization(64),\n","            bnc2=L.BatchNormalization(64),\n","            bnc3=L.BatchNormalization(128),\n","            bnc4=L.BatchNormalization(128),\n","            bnc5=L.BatchNormalization(256),\n","            bnc6=L.BatchNormalization(256),\n","            bnc7=L.BatchNormalization(512),\n","            bnc8=L.BatchNormalization(512),\n","\n","            bnd8=L.BatchNormalization(512),\n","            bnd7=L.BatchNormalization(256),\n","            bnd6=L.BatchNormalization(256),\n","            bnd5=L.BatchNormalization(128),\n","            bnd4=L.BatchNormalization(128),\n","            bnd3=L.BatchNormalization(64),\n","            bnd2=L.BatchNormalization(64),\n","            bnd1=L.BatchNormalization(32)\n","        )\n","\n","\n","    def forward(self, x):\n","\n","        e0 = F.relu(self.bnc0(self.c0(x)))\n","        e1 = F.relu(self.bnc1(self.c1(e0)))\n","        e2 = F.relu(self.bnc2(self.c2(e1)))\n","        del e1\n","        e3 = F.relu(self.bnc3(self.c3(e2)))\n","        e4 = F.relu(self.bnc4(self.c4(e3)))\n","        del e3\n","        e5 = F.relu(self.bnc5(self.c5(e4)))\n","        e6 = F.relu(self.bnc6(self.c6(e5)))\n","        del e5\n","        e7 = F.relu(self.bnc7(self.c7(e6)))\n","        e8 = F.relu(self.bnc8(self.c8(e7)))\n","\n","        d8 = F.relu(self.bnd8(self.dc8(F.concat([e7, e8]))))\n","        del e7, e8\n","        d7 = F.relu(self.bnd7(self.dc7(d8)))\n","        del d8\n","        d6 = F.relu(self.bnd6(self.dc6(F.concat([e6, d7]))))\n","        del d7, e6\n","        d5 = F.relu(self.bnd5(self.dc5(d6)))\n","        del d6\n","        d4 = F.relu(self.bnd4(self.dc4(F.concat([e4, d5]))))\n","        del d5, e4\n","        d3 = F.relu(self.bnd3(self.dc3(d4)))\n","        del d4\n","        d2 = F.relu(self.bnd2(self.dc2(F.concat([e2, d3]))))\n","        del d3, e2\n","        d1 = F.relu(self.bnd1(self.dc1(d2)))\n","        del d2\n","        d0 = self.dc0(F.concat([e0, d1]))\n","\n","        return d0\n","\n","\n","    def __call__(self, x, t):\n","\n","        h = self.forward(x)\n","        \n","        loss = F.softmax_cross_entropy(h, t, ignore_label=self.__ignore_label)\n","        accuracy = F.accuracy(h, t, ignore_label=self.__ignore_label)\n","        \n","        chainer.report({'loss': loss, 'accuracy': accuracy}, self)\n","        \n","        return loss\n","\n","        \n","    @property\n","    def class_num(self):\n","        return self.__class_num\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yts-TZ-trSfH","executionInfo":{"status":"ok","timestamp":1650722040016,"user_tz":-360,"elapsed":6400,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"93d7cc71-ec15-4aa1-bbad-edf996a7e015"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/chainer/_environment_check.py:75: UserWarning: \n","--------------------------------------------------------------------------------\n","CuPy (cupy-cuda111) version 9.4.0 may not be compatible with this version of Chainer.\n","Please consider installing the supported version by running:\n","  $ pip install 'cupy-cuda111>=7.7.0,<8.0.0'\n","\n","See the following page for more details:\n","  https://docs.cupy.dev/en/latest/install.html\n","--------------------------------------------------------------------------------\n","\n","  requirement=requirement, help=help))\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","import math\n","\n","import chainer\n","import chainer.functions as F\n","from chainer import cuda, serializers, Variable\n","\n","\n","class SegmentationModel:\n","\n","\tdef __init__(self, model_path, mean, gpu=0):\n","\n","\t\t# Load model\n","\t\tself.__model = UNet()\n","\t\tserializers.load_npz(model_path, self.__model)\n","\n","\t\tchainer.cuda.get_device(gpu).use()\n","\t\tself.__model.to_gpu(gpu)\n","\n","\t\t# Add height and width dimensions to mean \n","\t\tself.__mean = mean[np.newaxis, np.newaxis, :]\n","\n","\n","\tdef apply_segmentation(self, image):\n","\n","\t\timage_in, crop = self.__preprocess(image)\n","\n","\t\twith chainer.using_config('train', False):\n","\t\t\tscore = self.__model.forward(image_in)\n","\t\t\n","\t\tscore = F.softmax(score)\n","\t\tscore = cuda.to_cpu(score.data)[0]\n","\t\t\n","\t\ttop, left, bottom, right = crop\n","\t\tscore = score[:, top:bottom, left:right]\n","\t\t\n","\t\treturn score\n","\n","\n","\tdef apply_segmentation_to_mosaic(self, mosaic, grid_px=800, tile_overlap_px=200):\n","\n","\t\th, w, _ = mosaic.shape\n","\n","\t\tassert ((grid_px + tile_overlap_px * 2) % 16 == 0), \"(grid_px + tile_overlap_px * 2) must be divisible by 16\"\n","\n","\t\tpad_y1 = tile_overlap_px\n","\t\tpad_x1 = tile_overlap_px\n","\n","\t\tn_y = int(float(h) / float(grid_px))\n","\t\tn_x = int(float(w) / float(grid_px))\n","\t\tpad_y2 = n_y * grid_px + 2 * tile_overlap_px - h - pad_y1\n","\t\tpad_x2 = n_x * grid_px + 2 * tile_overlap_px - h - pad_x1\n","\n","\t\tmosaic_padded = np.pad(mosaic, ((pad_y1, pad_y2), (pad_x1, pad_x2), (0, 0)), 'symmetric')\n","\n","\t\tH, W, _ = mosaic_padded.shape\n","\t\tscore_padded = np.zeros(shape=[self.__model.class_num, H, W], dtype=np.float32)\n","\n","\t\tfor yi in range(n_y):\n","\t\t    for xi in range(n_x):\n","\t\t        \n","\t\t        top = yi * grid_px\n","\t\t        left = xi * grid_px\n","\t\t        bottom = top + grid_px + 2 * tile_overlap_px\n","\t\t        right = left + grid_px + 2 * tile_overlap_px\n","\t\t        \n","\t\t        tile = mosaic_padded[top:bottom, left:right]\n","\t\t        \n","\t\t        score_tile = self.apply_segmentation(tile)\n","\t\t        \n","\t\t        score_padded[:, top:bottom, left:right] = score_tile\n","\n","\t\tscore = score_padded[:, pad_y1:-pad_y2, pad_x1:-pad_x2]\n","\n","\t\treturn score\n","\n","\n","\tdef __preprocess(self, image):\n","\n","\t\th, w, _ = image.shape\n","\t\th_padded = int(math.ceil(float(h) / 16.0) * 16)\n","\t\tw_padded = int(math.ceil(float(w) / 16.0) * 16)\n","\n","\t\tpad_y1 = (h_padded - h) // 2\n","\t\tpad_x1 = (w_padded - w) // 2\n","\t\tpad_y2 = h_padded - h - pad_y1\n","\t\tpad_x2 = w_padded - w - pad_x1\n","\n","\t\timage_padded = np.pad(image, ((pad_y1, pad_y2), (pad_x1, pad_x2), (0, 0)), 'symmetric')\n","\t\timage_in = (image_padded - self.__mean) / 255.0\n","\t\timage_in = image_in.transpose(2, 0, 1)\n","\t\timage_in = image_in[np.newaxis, :, :, :]\n","\t\timage_in = Variable(cuda.cupy.asarray(image_in, dtype=cuda.cupy.float32))\n","\n","\t\ttop, left = pad_y1, pad_x1\n","\t\tbottom, right = top + h, left + w\n","\n","\t\treturn image_in, (top, left, bottom, right)\n"],"metadata":{"id":"2vjb2tEAqcCK","executionInfo":{"status":"ok","timestamp":1650722045348,"user_tz":-360,"elapsed":1036,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"v2tsBSM1ovWx","executionInfo":{"status":"ok","timestamp":1650722050684,"user_tz":-360,"elapsed":368,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}}},"outputs":[],"source":["from os import path\n","from PIL import Image\n","import numpy as np\n","from tqdm import tqdm\n","import sys\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"sN5ejnvTovWz","executionInfo":{"status":"ok","timestamp":1650722102710,"user_tz":-360,"elapsed":48539,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}}},"outputs":[],"source":["# Load trained model\n","# Modify the the paths based on your trained model location if needed.\n","mean = np.load('/content/drive/MyDrive/4101-AI-Project/dataset/mean.npy')\n","model = SegmentationModel('/content/drive/MyDrive/4101-AI-Project/models/logs/model_iter_3035', mean)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"gBGvB_tWovW0","executionInfo":{"status":"ok","timestamp":1650722107067,"user_tz":-360,"elapsed":387,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}}},"outputs":[],"source":["# Load test-set\n","# Modify the the paths based on your data location if needed.\n","with open('/content/drive/MyDrive/4101-AI-Project/dataset/test.txt') as f:\n","    lines = f.readlines()\n","\n","test_set = []\n","for line in lines:\n","    line = line.rstrip()\n","    test_set.append(line)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLrIibluovW2","executionInfo":{"status":"ok","timestamp":1650723190399,"user_tz":-360,"elapsed":1080106,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"b4e1d63d-c17f-44cc-8f26-2beb05092d2d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1389/1389 [17:59<00:00,  1.29it/s]\n"]}],"source":["#  Evaluate model\n","\n","# Modify the the paths based on your raster image and building label location.\n","image_dir = '/content/drive/MyDrive/4101-AI-Project/processedBuildingLabels/3band/3band'\n","label_dir = '/content/drive/MyDrive/4101-AI-Project/buildingMaskImages'\n","\n","gt_labels = []\n","pred_labels = []\n","\n","for test_data in tqdm(test_set):\n","    image_path = path.join(image_dir, test_data)\n","    label_path = path.join(label_dir, test_data)\n","    \n","    image = np.array(Image.open(image_path))\n","    label = np.array(Image.open(label_path))\n","    \n","    # Make gt_label, ground-truth building mask\n","    h, w = label.shape\n","    gt_label = np.zeros(shape=[h, w], dtype=np.int32) # 0: background\n","    gt_label[label > 0] = 1 # 1: \"building\"\n","    \n","    # Make pred_label, predicted building mask\n","    score = model.apply_segmentation(image)\n","    pred_label = np.argmax(score, axis=0)\n","    \n","    gt_labels.append(gt_label)\n","    pred_labels.append(pred_label)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jGFgjY6ovW3","executionInfo":{"status":"ok","timestamp":1650723313143,"user_tz":-360,"elapsed":4863,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"81dcffa8-65b6-4c74-9b0e-c81300d23278"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'iou': array([0.94615677, 0.50831679]), 'miou': 0.7272367773236843, 'pixel_accuracy': 0.9489958986034932, 'class_accuracy': array([0.9832502 , 0.59604563]), 'mean_class_accuracy': 0.7896479178727212}\n"]}],"source":["result = eval_semantic_segmentation(pred_labels, gt_labels)\n","print(result)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcnP8ZT5ovW5","executionInfo":{"status":"ok","timestamp":1650723320417,"user_tz":-360,"elapsed":371,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"e19633f6-75cf-40b4-af06-6d37fd5d90d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["IoU for class Building =  0.5083167860973677\n","Accuracy for class Building =  0.5960456345838827\n"]}],"source":["building_class = 1\n","print(\"IoU for class Building = \", result['iou'][building_class])\n","print(\"Accuracy for class Building = \", result['class_accuracy'][building_class])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvdwoURsovW7","executionInfo":{"status":"ok","timestamp":1650723328766,"user_tz":-360,"elapsed":4864,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"a58edfde-a8a1-4477-f042-1f94893db53a"},"outputs":[{"output_type":"stream","name":"stdout","text":["confusion:\n","[[221698815   3776669]\n"," [  8839613  13043089]]\n","\n","precision:  0.7754623461288801\n","recall:  0.5960456345838827\n"]}],"source":["confusion = calc_semantic_segmentation_confusion(pred_labels, gt_labels)\n","print(\"confusion:\")\n","print(confusion)\n","\n","tn = confusion[0][0]\n","fp = confusion[0][1]\n","tp = confusion[1][1]\n","fn = confusion[1][0]\n","\n","precision = float(tp) / float(tp + fp)\n","recall       = float(tp) / float(tp + fn)\n","\n","print()\n","print(\"precision: \", precision)\n","print(\"recall: \", recall)"]},{"cell_type":"markdown","source":["# Evaluate Other Model"],"metadata":{"id":"qZ_NPejfyBf4"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"8y0RBmBiovW8","executionInfo":{"status":"ok","timestamp":1650723481306,"user_tz":-360,"elapsed":3188,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}}},"outputs":[],"source":["model = SegmentationModel('/content/drive/MyDrive/4101-AI-Project/models/logs/model_iter_2428', mean)"]},{"cell_type":"code","source":["\n","gt_labels = []\n","pred_labels = []\n","\n","for test_data in tqdm(test_set):\n","    image_path = path.join(image_dir, test_data)\n","    label_path = path.join(label_dir, test_data)\n","    \n","    image = np.array(Image.open(image_path))\n","    label = np.array(Image.open(label_path))\n","    \n","    # Make gt_label, ground-truth building mask\n","    h, w = label.shape\n","    gt_label = np.zeros(shape=[h, w], dtype=np.int32) # 0: background\n","    gt_label[label > 0] = 1 # 1: \"building\"\n","    \n","    # Make pred_label, predicted building mask\n","    score = model.apply_segmentation(image)\n","    pred_label = np.argmax(score, axis=0)\n","    \n","    gt_labels.append(gt_label)\n","    pred_labels.append(pred_label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2eySxuCyVa_","executionInfo":{"status":"ok","timestamp":1650723755943,"user_tz":-360,"elapsed":249726,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"49b90737-8999-489a-a820-ff9a5ff4e5b3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1389/1389 [04:09<00:00,  5.57it/s]\n"]}]},{"cell_type":"code","source":["result = eval_semantic_segmentation(pred_labels, gt_labels)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uObK1pHUycRR","executionInfo":{"status":"ok","timestamp":1650723766815,"user_tz":-360,"elapsed":7554,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"ad2c70ab-5212-4677-d6bd-943d2cd219ad"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["{'iou': array([0.94734859, 0.5441041 ]), 'miou': 0.7457263445116342, 'pixel_accuracy': 0.9504615141380444, 'class_accuracy': array([0.97784374, 0.66831989]), 'mean_class_accuracy': 0.8230818161390682}\n"]}]},{"cell_type":"code","source":["building_class = 1\n","print(\"IoU for class Building = \", result['iou'][building_class])\n","print(\"Accuracy for class Building = \", result['class_accuracy'][building_class])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7awDRxXSyguC","executionInfo":{"status":"ok","timestamp":1650723786398,"user_tz":-360,"elapsed":434,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"20f9ded5-2e9e-471e-f95d-8fc9ac49c41a"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["IoU for class Building =  0.5441041029421586\n","Accuracy for class Building =  0.6683198902950833\n"]}]},{"cell_type":"code","source":["confusion = calc_semantic_segmentation_confusion(pred_labels, gt_labels)\n","print(\"confusion:\")\n","print(confusion)\n","\n","tn = confusion[0][0]\n","fp = confusion[0][1]\n","tp = confusion[1][1]\n","fn = confusion[1][0]\n","\n","precision = float(tp) / float(tp + fp)\n","recall       = float(tp) / float(tp + fn)\n","\n","print()\n","print(\"precision: \", precision)\n","print(\"recall: \", recall)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLViHdV1yjZ4","executionInfo":{"status":"ok","timestamp":1650723811474,"user_tz":-360,"elapsed":7542,"user":{"displayName":"Alif Khan","userId":"10908489260848847576"}},"outputId":"34e11c78-2abe-4bfb-cfaa-cab358edb631"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["confusion:\n","[[220479791   4995693]\n"," [  7258057  14624645]]\n","\n","precision:  0.745381909322867\n","recall:  0.6683198902950833\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"evaluate_model.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}